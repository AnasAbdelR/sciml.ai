<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <title> SciML Ecosystem Update: SDE Adjoints, FFORD Layers, and Jump Diffusion</title> <header> <h1><b><center>SciML Scientific Machine Learning Software</center></b></h1> <nav> | <a href="/" class=current >Home</a> | <a href="/news/">News</a> | <a href="/roadmap/">Roadmap</a> | <a href="/citing/">Citing</a> | <a href="/showcase/">Showcase</a> | <a href="/challenge/">Challenge Problems</a> | <a href="/community/">Community</a> <hr/> | <a href="/documentation/">Documentation</a> | <a href="/governance/">Governance</a> | <a href="https://benchmarks.sciml.ai/">Benchmarks</a> | <a href="https://github.com/SciML/">Source Code</a> </nav> </header> <div class=franklin-content ><p>This ecosystem update has a lot of stochastic components added. We have a new DSL and a bunch of new solvers which incorporate jump dynamics for Levy processes &#40;jump diffusions&#41;. Let&#39;s &quot;jump&quot; right in&#33;</p> <h2 id=catalystjl_chemical_reaction_models ><a href="#catalystjl_chemical_reaction_models">Catalyst.jl: Chemical Reaction Models</a></h2> <p><a href="https://catalyst.sciml.ai/dev/">Catalyst.jl</a> is our rebranding of the old DiffEqBio for its expanded role in chemical reaction modeling. You can easily design reaction networks and then simulate them with fast methods for jump equations:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Catlayst, DiffEqJump

rs = <span class=hljs-meta >@reaction_network</span> <span class=hljs-keyword >begin</span>
  c1, S + E --&gt; SE
  c2, SE --&gt; S + E
  c3, SE --&gt; P + E
<span class=hljs-keyword >end</span> c1 c2 c3
p = (<span class=hljs-number >0.00166</span>,<span class=hljs-number >0.0001</span>,<span class=hljs-number >0.1</span>)   <span class=hljs-comment ># [c1,c2,c3]</span>
tspan = (<span class=hljs-number >0.</span>, <span class=hljs-number >100.</span>)
u0 = [<span class=hljs-number >301.</span>, <span class=hljs-number >100.</span>, <span class=hljs-number >0.</span>, <span class=hljs-number >0.</span>]  <span class=hljs-comment ># [S,E,SE,P]</span>

<span class=hljs-comment ># solve JumpProblem</span>
dprob = DiscreteProblem(rs, u0, tspan, p)
jprob = JumpProblem(rs, dprob, Direct())
jsol = solve(jprob, SSAStepper())
plot(jsol,lw=<span class=hljs-number >2</span>,title=<span class=hljs-string >&quot;Gillespie: Michaelis-Menten Enzyme Kinetics&quot;</span>)</code></pre> <p><img src="https://user-images.githubusercontent.com/1814174/87864114-3bf9dd00-c932-11ea-83a0-58f38aee8bfb.png" alt="" /></p> <p>All of the current DifferentialEquations simulation features can be directly applied to Catalyst. Additionally, Catalyst is built on <a href="https://mtk.sciml.ai/dev/">ModelingToolkit.jl</a> so all of the <a href="https://youtu.be/UNkXNZZ3hSw">automatic optimization and parallelism features</a> can be directly applied to Catalyst generated code. A lot of the following features also extend continuous-time Markov models as well in new ways&#33;</p> <h2 id=adaptive_post-leap_tau_leaping ><a href="#adaptive_post-leap_tau_leaping">Adaptive Post-Leap Tau Leaping</a></h2> <p>Adaptive post-leap tau-leaping is here&#33; For example, an adaptive tau-leaping SIR can be written as:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> StochasticDiffEq, DiffEqJump, DiffEqBase, Statistics
<span class=hljs-keyword >using</span> Test, LinearAlgebra

<span class=hljs-keyword >function</span> regular_rate(out,u,p,t)
    out[<span class=hljs-number >1</span>] = (<span class=hljs-number >0.1</span>/<span class=hljs-number >1000.0</span>)*u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>]
    out[<span class=hljs-number >2</span>] = <span class=hljs-number >0.01</span>u[<span class=hljs-number >2</span>]
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >const</span> dc = zeros(<span class=hljs-number >3</span>, <span class=hljs-number >2</span>)
dc[<span class=hljs-number >1</span>,<span class=hljs-number >1</span>] = -<span class=hljs-number >1</span>
dc[<span class=hljs-number >2</span>,<span class=hljs-number >1</span>] = <span class=hljs-number >1</span>
dc[<span class=hljs-number >2</span>,<span class=hljs-number >2</span>] = -<span class=hljs-number >1</span>
dc[<span class=hljs-number >3</span>,<span class=hljs-number >2</span>] = <span class=hljs-number >1</span>

<span class=hljs-keyword >function</span> regular_c(du,u,p,t,counts,mark)
    mul!(du,dc,counts)
<span class=hljs-keyword >end</span>

rj = RegularJump(regular_rate,regular_c,<span class=hljs-number >2</span>)
jumps = JumpSet(rj)
iip_prob = DiscreteProblem([<span class=hljs-number >999.0</span>,<span class=hljs-number >1</span>,<span class=hljs-number >0</span>],(<span class=hljs-number >0.0</span>,<span class=hljs-number >250.0</span>))
jump_iipprob = JumpProblem(iip_prob,Direct(),rj)
<span class=hljs-meta >@time</span> sol = solve(jump_iipprob,TauLeaping())</code></pre> <p>This is compatible with all of the other parts of DifferentialEquations.jl like event handling, ensemble simulations, and more.</p> <h2 id=jump_diffusion_euler-maruyama_and_implicit_euler-maruyama ><a href="#jump_diffusion_euler-maruyama_and_implicit_euler-maruyama">Jump Diffusion Euler-Maruyama and Implicit Euler-Maruyama</a></h2> <p>Continuing with the theme of jump equations, Euler-Maruyama and Implicit Euler-Maruyama are now compatible with JumpProblem definitions, meaning that equations like jump diffusions can now be directly solved with non-adaptive jumps via Poisson additions. In the following we see the same discrete-jump SIR model, now with Euler-Maruyama:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> rate_oop(u,p,t)
    [(<span class=hljs-number >0.1</span>/<span class=hljs-number >1000.0</span>)*u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>],<span class=hljs-number >0.01</span>u[<span class=hljs-number >2</span>]]
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> regular_c(u,p,t,counts,mark)
    dc*counts
<span class=hljs-keyword >end</span>

rj = RegularJump(rate_oop,regular_c,<span class=hljs-number >2</span>)
foop(u,p,t) = [<span class=hljs-number >0.0</span>,<span class=hljs-number >0.0</span>,<span class=hljs-number >0.0</span>]
goop(u,p,t) = [<span class=hljs-number >0.0</span>,<span class=hljs-number >0.0</span>,<span class=hljs-number >0.0</span>]
oop_sdeprob = SDEProblem(foop,goop,[<span class=hljs-number >999.0</span>,<span class=hljs-number >1</span>,<span class=hljs-number >0</span>],(<span class=hljs-number >0.0</span>,<span class=hljs-number >250.0</span>))
jumpdiff_prob = JumpProblem(oop_sdeprob,Direct(),rj)
<span class=hljs-meta >@time</span> sol = solve(jumpdiff_prob,EM();dt=<span class=hljs-number >1.0</span>)</code></pre> <p>Now you see that there&#39;s <code>f</code> and <code>g</code> ready to be changed to mix and match continuous and discrete stochastic behaviors. This is an extension of our previous <a href="https://diffeq.sciml.ai/stable/tutorials/jump_diffusion/">jump diffusion support</a> by incorporating non-adapted jumping, allowing for scaling to jumps with higher rates.</p> <h2 id=adjoints_of_stochastic_differential_equations_and_new_sde_fitting_tutorials ><a href="#adjoints_of_stochastic_differential_equations_and_new_sde_fitting_tutorials">Adjoints of Stochastic Differential Equations and new SDE Fitting Tutorials</a></h2> <p>Stochastic differential equations now have <a href="https://frankschae.github.io/post/gsoc2020-high-weak-order-solvers-sde-adjoints/">adjoint definitions defined</a> which have extra optimizations on diagonal noise SDEs. This extends a <a href="https://gist.github.com/ChrisRackauckas/6a03e7b151c86b32d74b41af54d495c6">comparatively high performing SDE solver</a> to have low-memory SDE fitting. A <a href="https://diffeqflux.sciml.ai/dev/examples/optimization_sde/">new tutorial in DiffEqFlux</a> demonstrates how to recover the parameters of an SDE with just a few minutes of compute. Thank <a href="https://github.com/frankschae">@frankschae</a> for this advance&#33;</p> <h2 id=tons_of_methods_for_high_weak_order_solving_of_sdes ><a href="#tons_of_methods_for_high_weak_order_solving_of_sdes">Tons of methods for high weak order solving of SDEs</a></h2> <p>Due once again to <a href="https://github.com/frankschae">@frankschae</a>, we have plenty of new <a href="https://diffeq.sciml.ai/dev/solvers/sde_solve/#High-Weak-Order-Methods-1">high weak order methods</a> for fast solving of SDEs. When paired with <a href="https://github.com/SciML/DiffEqGPU.jl">recent performance improvements of DiffEqGPU</a>, we see some massive performance advantages for fitting expectations of equations. Formal benchmarks will come soon&#33;</p> <h2 id=continuous_normalizing_flows_and_ffjord_layers ><a href="#continuous_normalizing_flows_and_ffjord_layers">Continuous Normalizing Flows and FFJORD Layers</a></h2> <p>DiffEqFlux now provides <a href="https://diffeqflux.sciml.ai/dev/layers/CNFLayer/">pre-built continuous normalizing flow and FFJORD layers</a> for doing common neural ODE based machine learning. <a href="https://diffeqflux.sciml.ai/dev/examples/normalizing_flows/">A new tutorial</a> demonstrates how to get up and running with these layers in just a matter of minutes. Thank <a href="https://github.com/d-netto">Diogo Netto &#40;@d-netto&#41;</a> and <a href="https://github.com/avik-pal">Avik Pal &#40;@avik-pal&#41;</a> for these strong contributions to the SciML ecosystem&#33;</p> <h2 id=sparse_matrix_support_in_odesdedae_adjoints ><a href="#sparse_matrix_support_in_odesdedae_adjoints">Sparse Matrix Support in ODE/SDE/DAE Adjoints</a></h2> <p>The whole SciML ecosystem has already been making use of <a href="https://openreview.net/pdf?id&#61;rJlPdcY38B">automated sparsity tooling</a> and now it has improved. Now automated sparse differentiation is performed in the backpass of a differential equation adjoint system using the derived sparsity patterns. This greatly accelerates adjoints of stiff equations, like large partial differential equations. All that is required is for sparse differentiation to be used for the forward solve and the system will kick in and automatically derive and apply it to the reverse.</p> <h1 id=next_directions_google_summer_of_code ><a href="#next_directions_google_summer_of_code">Next Directions: Google Summer of Code</a></h1> <p>The next directions are going to be highly tied to the directions that we are going with the latest Google Summer of Code, so here are a few things to look forward to:</p> <ul> <li><p>Some tooling for automated training of physics-informed neural networks &#40;PINNs&#41; from ModelingToolkit symbolic descriptions of the PDE.</p> <li><p>More Lie Group integrator methods.</p> <li><p>Higher efficiency low-storage Runge-Kutta methods with a demonstration of optimality in a large-scale climate model &#40;&#33;&#33;&#33;&#41;.</p> <li><p>More high weak order methods for SDEs</p> <li><p>Causal components in ModelingToolkit</p> </ul> <p>And many many more. There will be enough that I don&#39;t think we will wait a whole month for the next update, so see you soon&#33;</p> <div class=page-foot > <div class=copyright > Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. </div> </div> </div>